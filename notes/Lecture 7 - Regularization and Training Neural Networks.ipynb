{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Recap of Initialization Techniques\n",
    "\n",
    "- For networks that are somewhat shallow, you can usually get away with initializing your weights with small, random values taken from a uniform or Gaussian distribution.\n",
    "- Large values almost never work, due to the exploding gradients problem.\n",
    "- For very deep networks, the issue is that the activations tend to go to zero at later layers in the network. Due to this, since the backpropagated gradients are scaled by the activations input into that layer, the gradients vanish and learning occurs very slowly, if at all.\n",
    "\n",
    "#### Xavier and He initialization\n",
    "- Xavier proposed the following initialization: $N(0, \\frac{2}{n_{in} + n_{out}}$. \n",
    "- This had a lot of assumptions, i.e. equivariance and linear units, so it didn't take into account the nonlinearity (usually ReLU). \n",
    "- As a result of this, the initialization ended up working decently for tanh, but would still fail for very deep neural networks that used the ReLU nonlinearity.\n",
    "- The he initialization was proposed to use the normalizer $/frac{2}{n_{in}}$ when initializing weights in networks that use the relu nonlinearity. THis is motivated by supposing that if linear activations prior to ReLU are equally likely to be positive or negative, then the ReLU would discard about half of them, decreasing the variance by a factor of 2, so we scale by 2 beforehand to maintain the same variance.\n",
    "\n",
    "#### Avoiding Saturation/High Variance Activations\n",
    "- As learning occurs, the distribution of inputs into successive layers change with respect to each other, leading to output activations that are highly variable. \n",
    "- So we know that these layers can change drastically, but when we do gradient descent and compute a $\\frac{dL}{dW}$, we are assuming that the other layers are kept the same (but they can change a lot!). THis leads to slower learning rates, more sensitivity to initalization, and saturation issues.\n",
    "\n",
    "#### Batchnorm\n",
    "- Make theoutput of each layer have unit Gaussian statistics\n",
    "- i.e. mean should be 0 and variance should be 1.\n",
    "- Therefore when we train, we are guaranteed that the input statistics into a layer remain roughly the same throughout training\n",
    "- Also makes the network less sensitive to initialization, and reduces the problems of exploding/vanishing gradients.\n",
    "- Normalize the activations: $$\\hat{x_i} = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$. \n",
    "- Then scale and shift: $$y_i = \\gamma_i * x_i + \\beta_i$$. \n",
    "- we can backpropagate gradients into these scale and shift parameters, which is useful because unit variance activations may not be what the next layer wants (for example if the next layer contains sigmoid neurons for some reason). \n",
    "- For example, the scale and shift operation could learn $\\gamma_i = \\sigma_i$ and $\\beta_i = \\mu_i$ in order to undo the effects of batch normalization.\n",
    "- Batchnorm is typically inserted after the affine layer, so instead of affine-relu we have affine-batchnorm-relu now. \n",
    "\n",
    "#### L2 regularization\n",
    "- Penalize the weights by their squared-L2 norm (Frobenius norms for matrices). Encourages smaller weight values/a simpler model. Gradient is just $\\frac{dL}{dW} \\frac{1}{2}||W||^2_F = W$\n",
    "\n",
    "#### L1 regularization\n",
    "- Penalize the weights by their absolute values: $ ||w||_1 = \\sum_i |w_i|$. Gradients: $-1$ if $w < 0$, $[-1, 1]$ if $w = 0$, and $+1$ if $ w > 0$.\n",
    "- Encourages sparse representations to be learned, can act like a feature selector. \n",
    "\n",
    "Ohter types of regularization: dataset augmentation, early stopping (via validation accuracy, which apparently can also act like a form of L2 regularization, see deep learning book), multitask learning\n",
    "\n",
    "#### Transfer Learning\n",
    "- Take a trained model and just do some fine-tuning on top of it. Useful when you have small datasets but want to leeverage the power of a trained InceptionV3 or something.\n",
    "\n",
    "#### Ensemble Methods\n",
    "- Train many different models and average their results togetehr.\n",
    "- $K$ indepdent models: the avergae model error decarases bey a factor $\\frac{1}{k}$.(In practice, the models aren't actually independent, so the improvement isn't this much, but emprically ensembles of neural networks have tended to do much better than a single network.\n",
    "- Ensemble methods are pretty powerful (they always win Kaggle competitions and won the Netflix prize) but usually they are impractical to use in a production setting due to inefficiency. \n",
    "- Lots of effort into approximation of ensemble methods: using the weights at local minima as different models and performing ensemble methods on top of that, so you can act like you have an ensemble of different weights that define different networks, but really you just took them from a single training -> snapshot ensembles. \n",
    "- Dropout can also be thought of as approximating ensemble methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
