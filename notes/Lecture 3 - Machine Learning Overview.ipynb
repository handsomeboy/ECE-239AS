{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of Linear Regression\n",
    "\n",
    "- Can extend a linear model to fit nonlinear functions\n",
    "\n",
    "#### Estimator bias and variance\n",
    "- $\\hat{\\theta} = (X^TX)^{-1}X^TY$\n",
    "- Train several models and get several different models\n",
    "- True underlying parameter $\\theta$, and our average estimation is $E[\\hat{\\theta}]$\n",
    "- The bias is $E[\\hat{theta}] - \\theta$\n",
    "- The variance is $E[(\\theta - E[\\hat{\\theta}])^2]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of bias and variance\n",
    "- Expectation is a linear operator\n",
    "- $E[aX] = aE[x] and E[X + Y] = E[X] + E[Y]$\n",
    "- $m$, the number of coin flips, doesn't affect the bias of the estimator, but it does affect the variance\n",
    "- i.e. if you flip a coin twice, you'll get a lot of variability, but if you flip it 3000 times, you're likely to get a more consistent distribution\n",
    "- when there's less data samples, the distribution is more likely to have high variability\n",
    "- Show that the sample mean estimator has bias 0\n",
    "\n",
    "#### Variance of sample mean estimator\n",
    "- Properties of variance: $var(aX) = a^2 var(X)$ and $var(x + y) = var(x) + var(y) + 2 cov(x, y)$, if x and y are ind\n",
    "\n",
    "#### Standard Error\n",
    "- More samples -> less variability in the mean\n",
    "- More samples -> smaller and smaller variance of your estimator\n",
    "- SE of the sample mean = $\\sqrt(var(\\hat{\\theta}))$\n",
    "\n",
    "#### MSE\n",
    "- Mean squared error = var(estimator) + bias(estimator)^2\n",
    "\n",
    "- Higher model complexity -> fit the data better\n",
    "- Have a lot more parameters/degrees of freedom\n",
    "- i.e. if you have $n$ points and are allowed to use $n-1$ parameters, you can just make a $n-1$ degree polynomial and fit the whole dataset\n",
    "\n",
    "\n",
    "#### Training, Validation, and testing data\n",
    "- Training data - used to laearn the params of your model\n",
    "- Validation data: used to optimize the hyperparameters \n",
    "- Testing data - score your model, use it only once, its the final accuracy\n",
    "- One method of selecting hyperparameters is to select some hyperparameters, train on the training data, and test on validation data, and select the hyperamraters that perform best on the validation data\n",
    "- This risks selecting a setting of hyperparameters that only work well on the validation dataset though. This is why we can also do k-fold CV\n",
    "\n",
    "#### Cross-Validation\n",
    "- Take your training data and divide it into $k$ folds \n",
    "- Train on $k-1$ and validate on the $k$th\n",
    "- Alternate this $k$ and take the average error. \n",
    "\n",
    "#### Maximum Likelihood\n",
    "- Find the params that yield the highest probability of having observed your dataset\n",
    "- Say we have 3 clusters and want to  model each cluster as being generated by a multivariate Gaussian. There are 3 Gaussian clusters, each one describes the distribution for the poits in that class. \n",
    "- We can use ML estimation to find the params for each cluster that maximizes our probability of observing the data\n",
    "- Basically we have 3 classes: $y_1, y_2, y_3$ and a bunch of data points $(x_i, y_i)$.\n",
    "- Each cluster is multivariate Gaussian, so it can be modelled by some mean and covariance matrix. \n",
    "- Write down the likelihood using independence assumption: $L = \\prod_{i=1}^{N} p(x_i,y_i)$ because we observe the pairs as coming from a joint model, and then use the chain rule of probability: $L = \\prod_{i=1}^{N} p(y_i) p(x_i | y_i)$. \n",
    "- Crucial assumption: $p(x_i | y_i) \\sim N(\\mu_i \\sigma_i) $\n",
    "- And then we can substitute this in to expand the likelihood, take the log, and maximize overall all 6 params (the means and covar matrices for each of the 3 classes). \n",
    "\n",
    "\n",
    "#### Chain Rule for probability\n",
    "- $p(a, b) = p(a | b) p(b) = p(a)p(b | a)$\n",
    "- $p(a, b, c) = p(a | b, c) p(b, c) = p(a | b, c) p(b | c) p(c)$\n",
    "- $p(b, c, d, e) = p(b, c | d, e) p(d, e) = p(b, c | d, e) p(e | d)p(d)$, so $p(b, c | d, e) = \\frac{p(b,c,d,e)}{p(d)p(e|d)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
