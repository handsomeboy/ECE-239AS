{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap of Transfer Learning\n",
    "- Take a trained network in one context and use it with little additional training\n",
    "- If you have a small dataset, just take inception v3 and add another linear layer on top of it.\n",
    "    - intuition is that if the tasks are similar, then features learned by inception on imagenet would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Methods\n",
    "- If the models are partially independent, less likely to make the same mistake\n",
    "- train multiple models and average their results together\n",
    "- bit more expensive though\n",
    "- wiht $k$ models, variability of error goes down by $\\frac{1}{k}$. \n",
    "- In general, assuming that the mean of the error is $0$ (i.e. unbiased). $$E[(\\frac{1}{k}\\sum_{i=1}^{k} \\epsilon_i)^2] = \\frac{1}{k}E[\\epsilon_i^2] + \\frac{k-1}{k}E[e_ie_j]$$\n",
    "\n",
    "#### Bagging\n",
    "- Ensemble method, for regularization- Construct $k$ datasets by sampling w/replacement\n",
    "- train $k$ different models\n",
    "- neural nets trained on the same dataset tend to produce partially independent errors because there's different init, hyperparameters, etc\n",
    "- more expensive though: time to train models can be very large, and also prediction time can go down. Unless you do it in parallel\n",
    "\n",
    "#### Other ideas\n",
    "- Huang et al used cosine annealing to find local minima and average them together\n",
    "\n",
    "#### Dropout\n",
    "- Method for generalization/regularization\n",
    "- Approximation of bagging procedure for exponentially many models\n",
    "- Procedure: sample 100 binary masks (draw 1 with p)\n",
    "- Apply the masks to all the units\n",
    "- Basically sets a proportion $1-p$ of all the activations to $0$. \n",
    "- approximating sparse structure\n",
    "- \"During training, dropout samples from an exponential number of different “thinned” networks\" - dropout paper.\n",
    "- Acts as an approximation of combining exponentially many ensemble learning/model combination, but with highly correlated networks\n",
    "- At test time, don't do dropout, but scale them by the probability of dropout $p$. (i.e. mult by $p$)\n",
    "- equivalent to dividing activations by $p$ during training.\n",
    "- Inverted dropout: divide the mask by $p$ while training\n",
    "- \"By doing this scaling, 2n networks with shared weights can be combined into a single neural network to be used at test time\" - dropout paper\n",
    "\n",
    "#### How can we make SGD even better? \n",
    "- Momentum, Adam, RMS prop, adaptive moments, second order methods\n",
    "- Regular SGD: $$\\theta \\leftarrow{} \\theta - \\epsilon * \\nabla_\\theta J(\\theta)$$\n",
    "- gradients are stochastic bc its a function of training data\n",
    "- smal batch sizes can act like a regularizer, because they introduce random noise into the training process.\n",
    "- noisily converging to a minimum\n",
    "- stochastic is good because it can also get you out of bad local minima\n",
    "- large learning rate causes zigzagging of gradients\n",
    "\n",
    "#### Momentum\n",
    "- Average gradient steps from previous iterations\n",
    "- Maintian running mean of the gradients, which then update the paramters\n",
    "- Set $v = 0, \\alpha \\in [0,1]$. Momentum update: $$v \\leftarrow{} \\alpha * v - \\epsilon*g$$ $$\\theta \\leftarrow{} \\theta + v$$. Basically it's $\\theta = \\theta + (\\alpha*v \\epsilon * g)$\n",
    "- implementing weighted average of past grads\n",
    "- Momentum can push you out of local optima, can push you out of local minima that is steep but not shallow, because it will still do updates since the gradient is $0$, but the momentum is not zero\n",
    "- Tends to converge to shallower optima, places tht have local curvature\n",
    "\n",
    "#### Nesterov Momentum\n",
    "- Evaluate teh gradient at $\\theta + \\alpha * v$. $$v \\leftarrow{} \\alpha * v - \\epsilon \\nabla_\\theta J(\\theta + \\alpha * v) $$ $$ \\theta \\leftarrow{} \\theta + v$$.\n",
    "- Intuition: compute the gradient with respect to what the paramters would be if you did only the momentum update. \n",
    "- Interpretation: since \n",
    "\n",
    "\n",
    "#### Adaptive Learning Rates\n",
    "- Adaptive Gradient: form of SGD where the LR is decreased thourh division by historical gradient norms. \n",
    "- Let $a = 0$ initially. Then while learning, compute the gradient $g$, and update $a \\leftarrow{} a + g\\circ g$, and the gradient step is $$\\theta \\leftarrow{} \\theta - \\frac{\\epsilon}{\\sqrt{a} + \\sigma} \\nabla_{\\theta} J$$\n",
    "- Basically decrease the learning rate in proportion to previous gradients, instead of just randomly decaying it\n",
    "- Issue: it remembers all of your past gradients, so if you ever had a huge gradient, then the learning rate will get scaled to be too small -> this is why Adagrad tends to slow down later in training\n",
    "\n",
    "#### RMSProp\n",
    "- Forget historical gradients\n",
    "- Augment Adagrad by making it an exponentially weighted moving average, forget initilay gradients \n",
    "- Just scale $a$ with $\\beta$ and $g \\circ g$ with $1 - \\beta$. Here $\\beta$ being small basically means that dividing the LR by past gradients is that much less important\n",
    "\n",
    "#### Adam\n",
    "- Adam with no bias correction: first and second moment\n",
    "- Adam with bias correction: adjust moments with bias exponentially with the timestep. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
